# Generating Audio with VAE

## Overview:
This project trains a variational autoencoder to generate new audio samples through an input of 20 dimensional vector.

This project is trained on the following dataset: [Audio Digits Dataset](https://github.com/Jakobovski/free-spoken-digit-dataset/tree/master/recordings)


## Overview of VAE:
- Unlike normal autoencoders, VAE tries to condense the data into a lower dimensional latent space with continuous distribution.
- VAE achieves this by considering a surrogate function q(z|x) which approximates to p(z|x), where z is the latent space and x in the input.
- After this, VAE finds the KL divergence of q(z|x) with p(z|x), which returns an evidence lower bound (ELBO) and log(p(z|x)).
- To achieve the optimal values of p(z|x) and p(x|z), we minimise the KL Divergence and maximise the ELBO.

## Overview of Data Preprocessing:

- This project uses audio data which is the be mapped on a latent space Z.
- The data is preprocess using the librosa library, where the data is converted into chunks of dft using stft.
- These chunks are than converted into spectrograms and stored in a numpy array. The VAE model trains on these spectrograms.

## Visualization of a spectrogram:

![image](https://github.com/AkshayKulkarni3467/Generative-Audio-VAE/assets/129979542/a5f26123-f142-424a-8ced-240fcb0bfdbc)


## Generated Audios:

- Audios can be achieved by : Inputting a spectrogram through the encoder-decoder architecture or forwarding a Z-dimensional vector through the decoder.
- The audios which are generated by inputting a spectrogram can be found at : samples/generated/





